{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "25013f1c",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Derivation of Backpropagation for Multiclass Classification\n",
    "\n",
    "This document details the step-by-step derivation of the backpropagation equations for a neural network layer performing multiclass classification. We assume a standard setup using Softmax activation and Categorical Cross-Entropy Loss.\n",
    "\n",
    "1. Notation and Setup\n",
    "\n",
    "Let's define our variables for a single training example with $N$ classes.\n",
    "\n",
    "$x$: Input vector of shape $(D \\times 1)$, where $D$ is the number of input features.\n",
    "\n",
    "$W$: Weight matrix of shape $(N \\times D)$.\n",
    "\n",
    "$b$: Bias vector of shape $(N \\times 1)$.\n",
    "\n",
    "$z$: The \"logits\" or linear output, shape $(N \\times 1)$.\n",
    "\n",
    "$a$: The activation (predicted probabilities) output by Softmax, shape $(N \\times 1)$.\n",
    "\n",
    "$y$: The ground truth label (one-hot encoded vector), shape $(N \\times 1)$.\n",
    "\n",
    "The Forward Pass Equations\n",
    "\n",
    "Linear Transformation:\n",
    "\n",
    "\n",
    "$$z_i = \\sum_{d=1}^{D} W_{id} x_d + b_i$$\n",
    "\n",
    "\n",
    "In vector form: $z = Wx + b$\n",
    "\n",
    "Softmax Activation:\n",
    "\n",
    "\n",
    "$$a_i = \\frac{e^{z_i}}{\\sum_{k=1}^{N} e^{z_k}}$$\n",
    "\n",
    "\n",
    "Note: $a_i$ is often denoted as $\\hat{y}_i$.\n",
    "\n",
    "Categorical Cross-Entropy Loss:\n",
    "\n",
    "\n",
    "$$L = -\\sum_{k=1}^{N} y_k \\log(a_k)$$\n",
    "\n",
    "\n",
    "Since $y$ is a one-hot vector (only one element is 1, the rest are 0), if the true class is $c$, this simplifies to $L = -\\log(a_c)$. However, we will use the summation form for the general derivation.\n",
    "\n",
    "2. The Goal\n",
    "\n",
    "To update the weights and biases using gradient descent, we need to find the partial derivatives of the Loss $L$ with respect to the weights $W$ and biases $b$:\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial W} \\quad \\text{and} \\quad \\frac{\\partial L}{\\partial b}$$\n",
    "\n",
    "Using the Chain Rule, we can break this down:\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial W} = \\frac{\\partial L}{\\partial z} \\cdot \\frac{\\partial z}{\\partial W}$$\n",
    "\n",
    "We will compute this layer by layer, starting from the loss and moving backward.\n",
    "\n",
    "3. Step-by-Step Derivation\n",
    "\n",
    "Step A: Derivative of Loss w.r.t. Softmax Output ($a$)\n",
    "\n",
    "First, we differentiate the loss function $L$ with respect to the $k$-th output activation $a_k$.\n",
    "\n",
    "$$L = -\\sum_{n=1}^{N} y_n \\log(a_n)$$\n",
    "\n",
    "Focusing on a specific class index $k$:\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial a_k} = -\\frac{\\partial}{\\partial a_k} \\left( y_k \\log(a_k) + \\sum_{n \\neq k} y_n \\log(a_n) \\right)$$\n",
    "\n",
    "Since the terms where $n \\neq k$ are constant with respect to $a_k$:\n",
    "\n",
    "$$\\boxed{\\frac{\\partial L}{\\partial a_k} = -\\frac{y_k}{a_k}}$$\n",
    "\n",
    "Step B: Derivative of Softmax ($a$) w.r.t. Logits ($z$)\n",
    "\n",
    "This is the trickiest part. The Softmax function relates every output $a_k$ to every input $z_j$ because of the denominator sum.\n",
    "\n",
    "$$a_k = \\frac{e^{z_k}}{\\Sigma} \\quad \\text{where} \\quad \\Sigma = \\sum_{n=1}^{N} e^{z_n}$$\n",
    "\n",
    "We need to find $\\frac{\\partial a_k}{\\partial z_j}$. There are two cases to consider using the quotient rule: $\\left( \\frac{u}{v} \\right)' = \\frac{u'v - uv'}{v^2}$.\n",
    "\n",
    "Case 1: $k = j$\n",
    "We are differentiating the output $a_j$ with respect to its own specific input $z_j$.\n",
    "\n",
    "\n",
    "$$\\frac{\\partial a_j}{\\partial z_j} = \\frac{(e^{z_j})(\\Sigma) - (e^{z_j})(e^{z_j})}{\\Sigma^2}$$\n",
    "\n",
    "$$= \\frac{e^{z_j}}{\\Sigma} \\cdot \\frac{\\Sigma - e^{z_j}}{\\Sigma} = a_j (1 - a_j)$$\n",
    "\n",
    "Case 2: $k \\neq j$\n",
    "We are differentiating an output $a_k$ with respect to a different input $z_j$. The numerator $e^{z_k}$ is constant w.r.t $z_j$.\n",
    "\n",
    "\n",
    "$$\\frac{\\partial a_k}{\\partial z_j} = \\frac{(0)(\\Sigma) - (e^{z_k})(e^{z_j})}{\\Sigma^2}$$\n",
    "\n",
    "$$= -\\frac{e^{z_k}}{\\Sigma} \\cdot \\frac{e^{z_j}}{\\Sigma} = -a_k a_j$$\n",
    "\n",
    "Summary of Step B:\n",
    "\n",
    "\n",
    "$$\\frac{\\partial a_k}{\\partial z_j} = \\begin{cases} a_j(1 - a_j) & \\text{if } k=j \\\\ -a_k a_j & \\text{if } k \\neq j \\end{cases}$$\n",
    "\n",
    "\n",
    "This can be written using the Kronecker delta $\\delta_{kj}$ (which is 1 if $k=j$ and 0 otherwise):\n",
    "\n",
    "\n",
    "$$\\boxed{\\frac{\\partial a_k}{\\partial z_j} = a_k (\\delta_{kj} - a_j)}$$\n",
    "\n",
    "Step C: Derivative of Loss ($L$) w.r.t. Logits ($z$)\n",
    "\n",
    "Now we combine Step A and Step B using the Chain Rule. We want the sensitivity of the Loss to a specific logit $z_j$. Since $z_j$ affects the Loss through all $N$ Softmax outputs, we must sum gradients from all $a_k$.\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial z_j} = \\sum_{k=1}^{N} \\frac{\\partial L}{\\partial a_k} \\cdot \\frac{\\partial a_k}{\\partial z_j}$$\n",
    "\n",
    "Substitute the results from Steps A and B:\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial z_j} = \\sum_{k=1}^{N} \\left( -\\frac{y_k}{a_k} \\right) \\cdot (a_k (\\delta_{kj} - a_j))$$\n",
    "\n",
    "Simplify the term inside the sum:\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial z_j} = -\\sum_{k=1}^{N} y_k (\\delta_{kj} - a_j)$$\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial z_j} = -\\left( \\sum_{k=1}^{N} y_k \\delta_{kj} - \\sum_{k=1}^{N} y_k a_j \\right)$$\n",
    "\n",
    "Let's analyze the two summation terms:\n",
    "\n",
    "$\\sum_{k=1}^{N} y_k \\delta_{kj}$: Since $\\delta_{kj}$ is 0 except when $k=j$, this sum collapses to simply $y_j$.\n",
    "\n",
    "$\\sum_{k=1}^{N} y_k a_j$: $a_j$ is constant w.r.t the sum index $k$, so we pull it out: $a_j \\sum_{k=1}^{N} y_k$. Since $y$ is a one-hot vector (probabilities sum to 1), $\\sum y_k = 1$. Thus, this term is $a_j$.\n",
    "\n",
    "Putting it back together:\n",
    "\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial z_j} = -(y_j - a_j)$$\n",
    "\n",
    "$$\\boxed{\\frac{\\partial L}{\\partial z_j} = a_j - y_j}$$\n",
    "\n",
    "This beautiful, simple result is why Softmax is nearly always paired with Cross-Entropy. The error signal is simply the difference between the predicted probability and the true label."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
